Copyright: 2019, Syed Raza, <raza220489@gmail.com>, <https://sites.google.com/view/sraza/>

Textbook: Reinforcement Learning, Second Edition
By Richard S. Sutton and Andrew G. Barto

1. Implement Q-learning. Complete q learn.py. (Make sure you have also completed sarsa.py from homework 4.) Then run cliff walking.py to test your code on the cliff walking example (see Example 6.6). You should get a figure similar to the one in Example 6.6. To get the learned paths under sarsa and q-learning, you may need to run the file several times. Append results as comments to q learn.py. Submit the generated figure and q learn.py.

7. Read section 6.7, Maximization Bias and Double Learning, from the textbook. Com- plete doubleQ.py. Then run max bias.py to test you code on the maximization bias example. Your figure should be similar to Figure 6.5. Submit the figure and dou- bleQ.py. 

8. Extra Credit: In max bias.py, we assumed that there were 12 actions available for state B (i.e., num actions B = 12), all of which lead to the left terminal state. Run the file again with num actions B = 30.
